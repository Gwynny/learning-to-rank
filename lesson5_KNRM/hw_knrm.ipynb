{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcda050e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gwyn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import math\n",
    "import nltk\n",
    "import string\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple, Union, Callable\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c860a1c",
   "metadata": {},
   "source": [
    "### 1. Preproccesing and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17177da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 handle_punctuation\n",
    "def handle_punctuation(inp_str: str) -> str:\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    new_str = inp_str.translate(translator)\n",
    "    return new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "292a799a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'return None'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_punctuation('return!None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "441e5c6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['return', 'None']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(handle_punctuation('return!None'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbeb544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 simple_preproc\n",
    "# delete punctuation -> lower -> tokenize\n",
    "def simple_preproc(inp_str: str) -> List[str]:\n",
    "    no_punctuation_str = handle_punctuation(inp_str)\n",
    "    lowered_str = no_punctuation_str.lower()\n",
    "    splitted_doc = nltk.word_tokenize(lowered_str)\n",
    "    return splitted_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9185d399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['return', 'none']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_preproc('return!None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efbb8d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 get_all_tokens and filter rare words\n",
    "\n",
    "def _filter_rare_words(vocab: Dict[str, int], min_occurancies: int) -> Dict[str, int]:\n",
    "    filtered_vocab = {x: count for x, count in vocab.items() if count >= min_occurancies}\n",
    "    return filtered_vocab\n",
    "\n",
    "def get_all_tokens(list_of_df: List[pd.DataFrame], min_occurancies: int) -> List[str]:\n",
    "    preped_series = []\n",
    "    for df in list_of_df:\n",
    "        preped_question1 = df['text_left'].apply(simple_preproc)\n",
    "        preped_question2 = df['text_right'].apply(simple_preproc)\n",
    "        preped_series.append(preped_question1)\n",
    "        preped_series.append(preped_question2)\n",
    "\n",
    "    concat_series = pd.concat(preped_series)\n",
    "    one_list_of_tokens = list(itertools.chain.from_iterable(concat_series.to_list()))\n",
    "    vocab = dict(Counter(one_list_of_tokens))\n",
    "    vocab = _filter_rare_words(vocab, min_occurancies)\n",
    "    return [key for key, _ in vocab.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "990377fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glue_df(glue_df) -> pd.DataFrame:\n",
    "    glue_df = glue_df.dropna(axis=0, how='any').reset_index(drop=True)\n",
    "    glue_df_fin = pd.DataFrame({\n",
    "        'id_left': glue_df['qid1'],\n",
    "        'id_right': glue_df['qid2'],\n",
    "        'text_left': glue_df['question1'],\n",
    "        'text_right': glue_df['question2'],\n",
    "        'label': glue_df['is_duplicate'].astype(int)\n",
    "    })\n",
    "    return glue_df_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58c6279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/QQP/train.tsv', sep='\\t')\n",
    "dev_df = pd.read_csv('data/QQP/dev.tsv', sep='\\t')\n",
    "train_df = get_glue_df(train_df)\n",
    "dev_df = get_glue_df(dev_df)\n",
    "list_of_df = [train_df, dev_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c04c7a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_left</th>\n",
       "      <th>id_right</th>\n",
       "      <th>text_left</th>\n",
       "      <th>text_right</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>213221</td>\n",
       "      <td>213222</td>\n",
       "      <td>How is the life of a math student? Could you d...</td>\n",
       "      <td>Which level of prepration is enough for the ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536040</td>\n",
       "      <td>536041</td>\n",
       "      <td>How do I control my horny emotions?</td>\n",
       "      <td>How do you control your horniness?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>364011</td>\n",
       "      <td>490273</td>\n",
       "      <td>What causes stool color to change to yellow?</td>\n",
       "      <td>What can cause stool to come out as little balls?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>155721</td>\n",
       "      <td>7256</td>\n",
       "      <td>What can one do after MBBS?</td>\n",
       "      <td>What do i do after my MBBS ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>279958</td>\n",
       "      <td>279959</td>\n",
       "      <td>Where can I find a power outlet for my laptop ...</td>\n",
       "      <td>Would a second airport in Sydney, Australia be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_left  id_right                                          text_left  \\\n",
       "0   213221    213222  How is the life of a math student? Could you d...   \n",
       "1   536040    536041                How do I control my horny emotions?   \n",
       "2   364011    490273       What causes stool color to change to yellow?   \n",
       "3   155721      7256                        What can one do after MBBS?   \n",
       "4   279958    279959  Where can I find a power outlet for my laptop ...   \n",
       "\n",
       "                                          text_right  label  \n",
       "0  Which level of prepration is enough for the ex...      0  \n",
       "1                 How do you control your horniness?      1  \n",
       "2  What can cause stool to come out as little balls?      0  \n",
       "3                       What do i do after my MBBS ?      1  \n",
       "4  Would a second airport in Sydney, Australia be...      0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3d749f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_tokens = get_all_tokens(list_of_df, min_occurancies=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25fcd740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how', 'is', 'the', 'life', 'of', 'a', 'math', 'student', 'could', 'you']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a46a873",
   "metadata": {},
   "source": [
    "### 2. Embedding matrix implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d179b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic retrieving of GLOVE embeddings from txt file\n",
    "# unk words replace with uniform vector with values inside [-0.2, 0.2]\n",
    "# return emb_matrix, vocabulary and unk_words\n",
    "\n",
    "def _read_glove_embeddings(file_path: str) -> Dict[str, List[str]]:\n",
    "    with open(file_path, encoding='utf-8') as file:\n",
    "        glove_dict = {}\n",
    "        for line in file:\n",
    "            splitted_line = line.split()\n",
    "            word, embedding = splitted_line[0], splitted_line[1:]\n",
    "            glove_dict[word] = embedding\n",
    "    return glove_dict\n",
    "    \n",
    "def create_glove_emb_from_file(file_path: str, inner_keys: List[str],\n",
    "                               random_seed: int, rand_uni_bound: float\n",
    "                               ) -> Tuple[np.ndarray, Dict[str, int], List[str]]:\n",
    "    np.random.seed(random_seed)\n",
    "    glove_dict = _read_glove_embeddings(file_path)\n",
    "    emb_dim = len(glove_dict['the'])\n",
    "    \n",
    "    emb_matrix = []\n",
    "    pad_vec = np.random.uniform(low=-rand_uni_bound, high=rand_uni_bound, size=emb_dim)\n",
    "    oov_vec = np.random.uniform(low=-rand_uni_bound, high=rand_uni_bound, size=emb_dim)\n",
    "    emb_matrix.append(pad_vec)\n",
    "    emb_matrix.append(oov_vec)\n",
    "    \n",
    "    vocab = {}\n",
    "    unk_words = []\n",
    "    vocab['PAD'], vocab['OOV'] = 0, 1\n",
    "    for ind, token in enumerate(inner_keys, 2):\n",
    "        if token in glove_dict.keys():\n",
    "            emb_matrix.append(glove_dict[token])\n",
    "            vocab[token] = ind\n",
    "        else:\n",
    "            unk_words.append(token)\n",
    "            vocab[token] = ind\n",
    "            random_emb = np.random.uniform(low=-rand_uni_bound, high=rand_uni_bound, size=emb_dim)\n",
    "            emb_matrix.append(random_emb)\n",
    "    emb_matrix = np.array(emb_matrix).astype(float)\n",
    "    return (emb_matrix, vocab, unk_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91d52d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix, vocab, unk_words = create_glove_emb_from_file('data/glove.6B.50d.txt', all_tokens, 0, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbc695d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e9e0001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26197"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unk_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9f1a0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87162"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61241eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87164"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec440292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3005552878547991"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unk_words) / len(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "524bab92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(87164, 50, padding_idx=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying Torch API\n",
    "emb_matrix = torch.nn.Embedding.from_pretrained(torch.FloatTensor(emb_matrix), freeze=True, padding_idx=0)\n",
    "emb_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cec31417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve 2 docs with 3 words example\n",
    "indices = torch.LongTensor([\n",
    "    [1, 33, 2],\n",
    "    [2, 4, 3]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20e21764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50])\n"
     ]
    }
   ],
   "source": [
    "print(emb_matrix(indices).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f5f0f3",
   "metadata": {},
   "source": [
    "### 3. Implementation of KNRM and Gaussian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "830687d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use gaussian kernel to get understanding if query or document is similar\n",
    "# We have cosine similarities from [-1, 1] and kernels with Mus across [-1, 1]\n",
    "# More words similar with each words in documents the bigger value we will have from\n",
    "# Kernels in ranges (0.7, 1] and lesser values in other kernels\n",
    "\n",
    "class GaussianKernel(torch.nn.Module):\n",
    "    def __init__(self, mu: float = 1., sigma: float = 1.):\n",
    "        super().__init__()\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, x):\n",
    "        numerator = -torch.pow((x - self.mu), 2)\n",
    "        denominator = 2 * self.sigma**2\n",
    "        return torch.exp(numerator / denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13d440db",
   "metadata": {},
   "outputs": [],
   "source": [
    "gk = GaussianKernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9974c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([[2, 3, 4],\n",
    "                  [1, 2, 3]]) \n",
    "b = torch.Tensor([[1, 1, 4],\n",
    "                  [1, 2, 3]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9c3de20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6065, 0.1353, 0.0111],\n",
       "        [1.0000, 0.6065, 0.1353]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gk(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22166ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42fd1b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.95, -0.85, -0.75, -0.65, -0.55, -0.45, -0.35, -0.25, -0.15,\n",
       "       -0.05,  0.05,  0.15,  0.25,  0.35,  0.45,  0.55,  0.65,  0.75,\n",
       "        0.85,  0.95,  1.  ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(np.linspace(-0.95, 0.95, 20), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90710c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(np.array([1,2] + [3]), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "662b2109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea in KNRM that we will have several Gaussian kernels with different MUs\n",
    "# We will match every word of query with every word of documents and then we will have matching matrix\n",
    "# Mij - cosine similarity between i word of query and j word of document \n",
    "# Applying Kernels to matching matrix we will likely know how query and document similar or dissimilar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6827e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNRM(torch.nn.Module):\n",
    "    def __init__(self, embedding_matrix: np.ndarray, freeze_embeddings: bool, kernel_num: int = 21,\n",
    "                 sigma: float = 0.1, exact_sigma: float = 0.001,\n",
    "                 out_layers: List[int] = [10, 5]):\n",
    "        super().__init__()\n",
    "        self.embeddings = torch.nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix),\n",
    "            freeze=freeze_embeddings,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        self.kernel_num = kernel_num\n",
    "        self.sigma = sigma\n",
    "        self.exact_sigma = exact_sigma\n",
    "        self.out_layers = out_layers\n",
    "\n",
    "        self.kernels = self._get_kernels_layers()\n",
    "\n",
    "        self.mlp = self._get_mlp()\n",
    "\n",
    "        self.out_activation = torch.nn.Sigmoid()\n",
    "\n",
    "    def _get_kernels_layers(self) -> torch.nn.ModuleList:\n",
    "        kernels = torch.nn.ModuleList()\n",
    "        # my code here\n",
    "        shrink_len = 1.0 /  (self.kernel_num - 1)\n",
    "        left, right = -1.0 + shrink_len, 1.0 - shrink_len\n",
    "        mus = np.append(np.linspace(left, right, self.kernel_num-1), 1.0)\n",
    "        sigmas = np.array((self.kernel_num-1)*[self.sigma] + [self.exact_sigma])\n",
    "        \n",
    "        for mu, sigma in zip(mus, sigmas):\n",
    "            kernels.append(GaussianKernel(mu=mu, sigma=sigma))\n",
    "        return kernels\n",
    "\n",
    "    def _get_mlp(self) -> torch.nn.Sequential:\n",
    "       # my code here\n",
    "        if len(self.out_layers) == 0:\n",
    "            return torch.nn.Sequential(torch.nn.Linear(self.kernel_num, 1))\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(torch.nn.Linear(self.kernel_num, self.out_layers[0]))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        for i in range(1, len(self.out_layers)):\n",
    "            layers.append(torch.nn.Linear(self.out_layers[i-1], self.out_layers[i]))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "        layers.append(torch.nn.Linear(self.out_layers[-1], 1))\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input_1: Dict[str, torch.Tensor], input_2: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n",
    "        logits_1 = self.predict(input_1)\n",
    "        logits_2 = self.predict(input_2)\n",
    "\n",
    "        logits_diff = logits_1 - logits_2\n",
    "\n",
    "        out = self.out_activation(logits_diff)\n",
    "        return out\n",
    "\n",
    "    def _get_matching_matrix(self, query: torch.Tensor, doc: torch.Tensor) -> torch.FloatTensor:\n",
    "        # my code here\n",
    "        # https://stackoverflow.com/questions/50411191/\n",
    "        # how-to-compute-the-cosine-similarity-in-pytorch-for-all-rows-in-a-matrix-with-re\n",
    "        eps = 1e-8\n",
    "        query_m, doc_m = self.embeddings(query), self.embeddings(doc)\n",
    "        query_norm, doc_norm = query_m.norm(dim=2)[:, :, None], doc_m.norm(dim=2)[:, :, None]\n",
    "        query_normalised = query_m / torch.clamp(query_norm, min=eps)\n",
    "        doc_normalised = doc_m / torch.clamp(doc_norm, min=eps)\n",
    "        similarity_m = torch.bmm(query_normalised, doc_normalised.transpose(1, 2))\n",
    "        return similarity_m\n",
    "\n",
    "    def _apply_kernels(self, matching_matrix: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        KM = []\n",
    "        for kernel in self.kernels:\n",
    "            # shape = [B]\n",
    "            K = torch.log1p(kernel(matching_matrix).sum(dim=-1)).sum(dim=-1)\n",
    "            KM.append(K)\n",
    "\n",
    "        # shape = [B, K]\n",
    "        kernels_out = torch.stack(KM, dim=1)\n",
    "        return kernels_out\n",
    "\n",
    "    def predict(self, inputs: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n",
    "        # shape = [Batch, Left], [Batch, Right]\n",
    "        query, doc = inputs['query'], inputs['document']\n",
    "\n",
    "        # shape = [Batch, Left, Right]\n",
    "        matching_matrix = self._get_matching_matrix(query, doc)\n",
    "        # shape = [Batch, Kernels]\n",
    "        kernels_out = self._apply_kernels(matching_matrix)\n",
    "        # shape = [Batch]\n",
    "        out = self.mlp(kernels_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3cb005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix, vocab, unk_words = create_glove_emb_from_file('data/glove.6B.50d.txt', all_tokens, 0, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1fe7bc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4e7f046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0195254 ,  0.08607575,  0.04110535, ..., -0.14842948,\n",
       "        -0.07382866, -0.05451569],\n",
       "       [ 0.02807871, -0.02455939,  0.19534954, ..., -0.19195698,\n",
       "         0.13157601, -0.19812181],\n",
       "       [ 0.68938   , -0.10644   ,  0.17083   , ...,  0.41761   ,\n",
       "        -0.22504   ,  0.61412   ],\n",
       "       ...,\n",
       "       [-0.60955   ,  0.62538   , -0.035572  , ...,  1.4036    ,\n",
       "         0.81419   ,  0.097825  ],\n",
       "       [-0.17766197,  0.16951815,  0.13465873, ..., -0.08008661,\n",
       "        -0.11563979,  0.12498598],\n",
       "       [ 0.11233   ,  1.4166    , -1.0127    , ...,  0.012063  ,\n",
       "        -1.0092    , -0.37959   ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_matrix.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dcf41d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "knrm = KNRM(emb_matrix, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "484d17e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=21, out_features=10, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knrm._get_mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55916f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch \n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30157caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.9245,  0.8041],\n",
       "          [ 0.8165,  0.7707]],\n",
       " \n",
       "         [[-0.6472,  0.9513],\n",
       "          [-0.6929,  0.9723]]]),\n",
       " torch.Size([2, 2, 2]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor(\n",
    "    [[[1, 3, 4],\n",
    "     [1, 1, 1]],\n",
    "     [[1, 3, 4],\n",
    "     [1, 2, 3]]]\n",
    "    )\n",
    "b = torch.Tensor(\n",
    "    [[[1, 1, 4],\n",
    "     [1, 10, 3]],\n",
    "     [[-1, 1, -4],\n",
    "     [2, 2, 3]]]\n",
    "     )\n",
    "eps = 1e-8\n",
    "a_n, b_n = a_n, b_n = a.norm(dim=2)[:, :, None], b.norm(dim=2)[:, :, None]\n",
    "a_norm = a / torch.clamp(a_n, min=eps)\n",
    "b_norm = b / torch.clamp(b_n, min=eps)\n",
    "sim_mt = torch.bmm(a_norm, b_norm.transpose(1, 2))\n",
    "sim_mt, sim_mt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9314fd02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9245,  0.8041],\n",
       "         [ 0.8165,  0.7707]],\n",
       "\n",
       "        [[-0.6472,  0.9513],\n",
       "         [-0.6929,  0.9723]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_matrix = torch.einsum(\n",
    "    'bld,brd->blr',\n",
    "    torch.nn.functional.normalize(a, p=2, dim=-1),\n",
    "    torch.nn.functional.normalize(b, p=2, dim=-1)\n",
    ")\n",
    "matching_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c239f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9effef6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8040544114072732"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check similarity of first word of query and second word of document\n",
    "aa = np.array([1, 3, 4])\n",
    "bb = np.array([1, 10, 3])\n",
    "cos_sim = np.dot(aa, bb)/(np.linalg.norm(aa)*np.linalg.norm(bb))\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9a5e512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_kernels_layers() -> torch.nn.ModuleList:\n",
    "    kernels = torch.nn.ModuleList()\n",
    "    # my code here\n",
    "    shrink_len = 1.0 / 10\n",
    "    left, right = -1.0 + shrink_len, 1.0 - shrink_len\n",
    "    mus = np.append(np.linspace(left, right, 10), 1.0)\n",
    "\n",
    "    for mu in mus:\n",
    "        kernels.append(GaussianKernel(mu=mu))\n",
    "\n",
    "    return kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b63bd60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = _get_kernels_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5089953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_kernels(kernels, matching_matrix: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    KM = []\n",
    "    for kernel in kernels:\n",
    "        # shape = [B]\n",
    "        K = torch.log1p(kernel(matching_matrix).sum(dim=-1)).sum(dim=-1)\n",
    "        KM.append(K)\n",
    "    \n",
    "    # shape = [B, K]\n",
    "    kernels_out = torch.stack(KM, dim=1)\n",
    "    return kernels_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1db255dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.,  1.,  4.],\n",
       "          [ 1., 10.,  3.]],\n",
       " \n",
       "         [[-1.,  1., -4.],\n",
       "          [ 2.,  2.,  3.]]]),\n",
       " tensor([[[ 0.9245,  0.8041],\n",
       "          [ 0.8165,  0.7707]],\n",
       " \n",
       "         [[-0.6472,  0.9513],\n",
       "          [-0.6929,  0.9723]]]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, sim_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "10edf336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels[-1].mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "18616fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9997, 0.9954],\n",
       "         [0.9965, 0.9917]],\n",
       "\n",
       "        [[0.3021, 0.9987],\n",
       "         [0.2812, 0.9974]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels[-2](sim_mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "684d2837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4234, 0.4769],\n",
       "        [1.1487, 1.1521]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels[0](sim_mt).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e0802934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3531, 0.3899],\n",
       "        [0.7649, 0.7664]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log1p(kernels[0](sim_mt).sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e29807f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7430, 1.5313])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log1p(kernels[0](sim_mt).sum(dim=-1)).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "601f151f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7430, 0.9680, 1.2062, 1.4431, 1.6645, 1.8580, 2.0135, 2.1238, 2.1840,\n",
       "         2.1916, 2.1756],\n",
       "        [1.5313, 1.6225, 1.6908, 1.7381, 1.7666, 1.7783, 1.7741, 1.7536, 1.7152,\n",
       "         1.6568, 1.6194]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_apply_kernels(kernels, sim_mt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe24a3d",
   "metadata": {},
   "source": [
    "### 4. Implementation of Dataset and Dataloader for train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3404009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_to_text_mapping(inp_df: pd.DataFrame) -> Dict[str, str]:\n",
    "    left_dict = (\n",
    "        inp_df[\n",
    "            ['id_left', 'text_left']\n",
    "        ].drop_duplicates()\n",
    "        .set_index('id_left')\n",
    "        ['text_left'].to_dict()\n",
    "    )\n",
    "    right_dict = (\n",
    "        inp_df[\n",
    "            ['id_right', 'text_right']\n",
    "        ].drop_duplicates()\n",
    "        .set_index('id_right')\n",
    "        ['text_right'].to_dict()\n",
    "    )\n",
    "    left_dict.update(right_dict)\n",
    "    return left_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a152c5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(213221,\n",
       "  'How is the life of a math student? Could you describe your own experiences?'),\n",
       " (536040, 'How do I control my horny emotions?'),\n",
       " (364011, 'What causes stool color to change to yellow?'),\n",
       " (155721, 'What can one do after MBBS?'),\n",
       " (279958,\n",
       "  'Where can I find a power outlet for my laptop at Melbourne Airport?')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = get_idx_to_text_mapping(train_df)\n",
    "list(d.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb0f4803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_left</th>\n",
       "      <th>id_right</th>\n",
       "      <th>text_left</th>\n",
       "      <th>text_right</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>213221</td>\n",
       "      <td>213222</td>\n",
       "      <td>How is the life of a math student? Could you d...</td>\n",
       "      <td>Which level of prepration is enough for the ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536040</td>\n",
       "      <td>536041</td>\n",
       "      <td>How do I control my horny emotions?</td>\n",
       "      <td>How do you control your horniness?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>364011</td>\n",
       "      <td>490273</td>\n",
       "      <td>What causes stool color to change to yellow?</td>\n",
       "      <td>What can cause stool to come out as little balls?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>155721</td>\n",
       "      <td>7256</td>\n",
       "      <td>What can one do after MBBS?</td>\n",
       "      <td>What do i do after my MBBS ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>279958</td>\n",
       "      <td>279959</td>\n",
       "      <td>Where can I find a power outlet for my laptop ...</td>\n",
       "      <td>Would a second airport in Sydney, Australia be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_left  id_right                                          text_left  \\\n",
       "0   213221    213222  How is the life of a math student? Could you d...   \n",
       "1   536040    536041                How do I control my horny emotions?   \n",
       "2   364011    490273       What causes stool color to change to yellow?   \n",
       "3   155721      7256                        What can one do after MBBS?   \n",
       "4   279958    279959  Where can I find a power outlet for my laptop ...   \n",
       "\n",
       "                                          text_right  label  \n",
       "0  Which level of prepration is enough for the ex...      0  \n",
       "1                 How do you control your horniness?      1  \n",
       "2  What can cause stool to come out as little balls?      0  \n",
       "3                       What do i do after my MBBS ?      1  \n",
       "4  Would a second airport in Sydney, Australia be...      0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9a5f97d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### walkthrough def create_val_pairs. Can be found in solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db0f731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_df_select = train_df[['id_left', 'id_right', 'label']]\n",
    "inf_df_group_sizes = inp_df_select.groupby('id_left').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b52fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_group_size=2\n",
    "glue_dev_leftids_to_use = list(inf_df_group_sizes[inf_df_group_sizes >= min_group_size].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0cf966ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47573"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glue_dev_leftids_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e9d0f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = inp_df_select[inp_df_select.id_left.isin(glue_dev_leftids_to_use)].groupby('id_left')\n",
    "all_ids = set(train_df['id_left']).union(set(train_df['id_right']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9001ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_left, group in groups:\n",
    "    id_left, grp = id_left, group\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "778de7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id_left  id_right  label\n",
      "37749         3    282170      0\n",
      "175701        3    488853      0\n",
      "297478        3         4      0\n",
      "317731        3    380197      0\n"
     ]
    }
   ],
   "source": [
    "print(grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fb65792e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([282170, 488853,      4, 380197], dtype=int64))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones_ids = group[group.label > 0].id_right.values\n",
    "zeroes_ids = group[group.label == 0].id_right.values\n",
    "ones_ids, zeroes_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1b6c49ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_len = len(ones_ids) + len(zeroes_ids)\n",
    "num_pad_items = max(0, 15 - sum_len)\n",
    "num_pad_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3632b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_pad_items > 0:\n",
    "    cur_chosen = set(ones_ids).union(\n",
    "        set(zeroes_ids)).union({id_left})\n",
    "    pad_sample = np.random.choice(\n",
    "        list(all_ids - cur_chosen), num_pad_items,\n",
    "        replace=False).tolist()\n",
    "else:\n",
    "    pad_sample = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a79afe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pairs = []\n",
    "for i in ones_ids:\n",
    "    out_pairs.append([id_left, i, 2])\n",
    "for i in zeroes_ids:\n",
    "    out_pairs.append([id_left, i, 1])\n",
    "for i in pad_sample:\n",
    "    out_pairs.append([id_left, i, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b74591ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 282170, 1],\n",
       " [3, 488853, 1],\n",
       " [3, 4, 1],\n",
       " [3, 380197, 1],\n",
       " [3, 476710, 0],\n",
       " [3, 486530, 0],\n",
       " [3, 334570, 0],\n",
       " [3, 249341, 0],\n",
       " [3, 56180, 0],\n",
       " [3, 254444, 0],\n",
       " [3, 158257, 0],\n",
       " [3, 431212, 0],\n",
       " [3, 120488, 0],\n",
       " [3, 264336, 0],\n",
       " [3, 343946, 0]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06867b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, index_pairs_or_triplets: List[List[Union[str, float]]],\n",
    "                 idx_to_text_mapping: Dict[int, str],\n",
    "                 vocab: Dict[str, int],\n",
    "                 oov_val: int,\n",
    "                 preproc_func: Callable, max_len: int = 30):\n",
    "        self.index_pairs_or_triplets = index_pairs_or_triplets\n",
    "        self.idx_to_text_mapping = idx_to_text_mapping\n",
    "        self.vocab = vocab\n",
    "        self.oov_val = oov_val\n",
    "        self.preproc_func = preproc_func\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_pairs_or_triplets)\n",
    "\n",
    "    def _tokenized_text_to_index(self, tokenized_text: List[str]) -> List[int]:\n",
    "        # my code here\n",
    "        token_idxs = []\n",
    "        text = tokenized_text[:self.max_len]\n",
    "        for token in text:\n",
    "            token_idxs.append(self.vocab.get(token, self.oov_val))\n",
    "        return token_idxs\n",
    "\n",
    "    def _convert_text_idx_to_token_idxs(self, idx: int) -> List[int]:\n",
    "        # my code here\n",
    "        text = self.idx_to_text_mapping[idx]\n",
    "        tokenized_text = self.preproc_func(text)\n",
    "        token_idxs = self._tokenized_text_to_index(tokenized_text)\n",
    "        return token_idxs\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "27e00105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(213221,\n",
       "  'How is the life of a math student? Could you describe your own experiences?'),\n",
       " (536040, 'How do I control my horny emotions?'),\n",
       " (364011, 'What causes stool color to change to yellow?'),\n",
       " (155721, 'What can one do after MBBS?'),\n",
       " (279958,\n",
       "  'Where can I find a power outlet for my laptop at Melbourne Airport?')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(d.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "856e9a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 282170, 1], [3, 488853, 1], [3, 4, 1]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_pairs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "60cbb40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out_pairs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "475520eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a6762d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(list(range(3)), 1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fd0f66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTripletsDataset(RankingDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        # my code here\n",
    "        triplets = self.index_pairs_or_triplets[idx]\n",
    "        query_tokens = self._convert_text_idx_to_token_idxs(str(triplets[0]))\n",
    "        left_doc_tokens = self._convert_text_idx_to_token_idxs(str(triplets[1]))\n",
    "        right_doc_tokens = self._convert_text_idx_to_token_idxs(str(triplets[2]))\n",
    "        label = triplets[3]\n",
    "\n",
    "        left_query_doc = {'query': query_tokens, 'document': left_doc_tokens}\n",
    "        right_query_doc = {'query': query_tokens, 'document': right_doc_tokens}\n",
    "        return left_query_doc, right_query_doc, label\n",
    "\n",
    "\n",
    "class ValPairsDataset(RankingDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        # my code here\n",
    "        pairs = self.index_pairs_or_triplets[idx]\n",
    "        query_tokens = self._convert_text_idx_to_token_idxs(str(pairs[0]))\n",
    "        doc_tokens = self._convert_text_idx_to_token_idxs(str(pairs[1]))\n",
    "        label = pairs[2]\n",
    "\n",
    "        query_doc = {'query': query_tokens, 'document': doc_tokens}\n",
    "        return query_doc, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1843c56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_left</th>\n",
       "      <th>id_right</th>\n",
       "      <th>text_left</th>\n",
       "      <th>text_right</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>213221</td>\n",
       "      <td>213222</td>\n",
       "      <td>How is the life of a math student? Could you d...</td>\n",
       "      <td>Which level of prepration is enough for the ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536040</td>\n",
       "      <td>536041</td>\n",
       "      <td>How do I control my horny emotions?</td>\n",
       "      <td>How do you control your horniness?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>364011</td>\n",
       "      <td>490273</td>\n",
       "      <td>What causes stool color to change to yellow?</td>\n",
       "      <td>What can cause stool to come out as little balls?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>155721</td>\n",
       "      <td>7256</td>\n",
       "      <td>What can one do after MBBS?</td>\n",
       "      <td>What do i do after my MBBS ?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>279958</td>\n",
       "      <td>279959</td>\n",
       "      <td>Where can I find a power outlet for my laptop ...</td>\n",
       "      <td>Would a second airport in Sydney, Australia be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_left  id_right                                          text_left  \\\n",
       "0   213221    213222  How is the life of a math student? Could you d...   \n",
       "1   536040    536041                How do I control my horny emotions?   \n",
       "2   364011    490273       What causes stool color to change to yellow?   \n",
       "3   155721      7256                        What can one do after MBBS?   \n",
       "4   279958    279959  Where can I find a power outlet for my laptop ...   \n",
       "\n",
       "                                          text_right  label  \n",
       "0  Which level of prepration is enough for the ex...      0  \n",
       "1                 How do you control your horniness?      1  \n",
       "2  What can cause stool to come out as little balls?      0  \n",
       "3                       What do i do after my MBBS ?      1  \n",
       "4  Would a second airport in Sydney, Australia be...      0  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a3a2920e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_left</th>\n",
       "      <th>id_right</th>\n",
       "      <th>text_left</th>\n",
       "      <th>text_right</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37749</th>\n",
       "      <td>3</td>\n",
       "      <td>282170</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>Is it possible to melt down diamonds?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175701</th>\n",
       "      <td>3</td>\n",
       "      <td>488853</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>Could India keep the Koh-I-Noor safe?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297478</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317731</th>\n",
       "      <td>3</td>\n",
       "      <td>380197</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What are some interesting facts about Kohinoor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_left  id_right                                          text_left  \\\n",
       "37749         3    282170  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "175701        3    488853  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "297478        3         4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "317731        3    380197  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "\n",
       "                                               text_right  label  \n",
       "37749               Is it possible to melt down diamonds?      0  \n",
       "175701              Could India keep the Koh-I-Noor safe?      0  \n",
       "297478  What would happen if the Indian government sto...      0  \n",
       "317731  What are some interesting facts about Kohinoor...      0  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df['id_left']==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "595e8668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data_for_train_iter(inp_df: pd.DataFrame, seed: int\n",
    "                               ) -> List[List[Union[str, float]]]:\n",
    "    inp_df_select = train_df[['id_left', 'id_right', 'label']]\n",
    "    inf_df_group_sizes = inp_df_select.groupby('id_left').size()\n",
    "    glue_dev_leftids_to_use = list(inf_df_group_sizes[inf_df_group_sizes >= 3].index)\n",
    "    glue_dev_leftids_to_use = np.random.choice(list(glue_dev_leftids_to_use), size=3000, replace=False)\n",
    "    groups = inp_df_select[inp_df_select.id_left.isin(\n",
    "        glue_dev_leftids_to_use)].groupby('id_left')\n",
    "\n",
    "    all_ids = set(train_df['id_left']).union(set(train_df['id_right']))\n",
    "\n",
    "    out_triplets = []\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    negative_example = np.random.choice(list(all_ids), size=1).item()\n",
    "    for id_left, group in groups:\n",
    "        right_ids = np.array(group['id_right'].to_list())\n",
    "        np.random.shuffle(right_ids)\n",
    "        all_groups_ids = set([id_left]).union(set(right_ids))\n",
    "        candidates = list(itertools.combinations(right_ids, 2))\n",
    "        candidates_inds = np.random.choice(list(range(len(candidates))), size=3, replace=False)\n",
    "        \n",
    "        for ind in candidates_inds:\n",
    "            candidate_left, candidate_right = candidates[ind][0], candidates[ind][1]\n",
    "            left_label = group[group['id_right']==candidate_left]['label'].item()\n",
    "            right_label = group[group['id_right']==candidate_right]['label'].item()\n",
    "            label_diff = left_label - right_label\n",
    "            if label_diff > 0:\n",
    "                out_triplets.append([id_left, candidate_left, candidate_right, 1])\n",
    "            else:\n",
    "                out_triplets.append([id_left, candidate_left, candidate_right, 0])\n",
    "                \n",
    "        #negative_example = np.random.choice(list(all_ids), size=1).item()\n",
    "        out_triplets.append([id_left, candidate_right, negative_example, 0])\n",
    "        negative_example = id_left\n",
    "    out_triplets = np.array(out_triplets)\n",
    "    #train_inds = np.random.choice(list(range(len(out_triplets))), size=10000, replace=False)\n",
    "    return out_triplets#[train_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2e608d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_left</th>\n",
       "      <th>id_right</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37749</th>\n",
       "      <td>3</td>\n",
       "      <td>282170</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175701</th>\n",
       "      <td>3</td>\n",
       "      <td>488853</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297478</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317731</th>\n",
       "      <td>3</td>\n",
       "      <td>380197</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_left  id_right  label\n",
       "37749         3    282170      0\n",
       "175701        3    488853      0\n",
       "297478        3         4      0\n",
       "317731        3    380197      0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8f1082f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = sample_data_for_train_iter(train_df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "20e28be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    51,     52, 161854,      0],\n",
       "       [    51,     52, 291074,      0],\n",
       "       [    51,     52, 102258,      0],\n",
       "       ...,\n",
       "       [493505, 354541, 128937,      0],\n",
       "       [493505, 354541,  44893,      0],\n",
       "       [493505,  44893, 475986,      0]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5088a900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int32"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(triplets[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041dbf59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bf54b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aecad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(\n",
    "        batch_objs: List[Union[Dict[str, torch.Tensor], torch.FloatTensor]]):\n",
    "    max_len_q1 = -1\n",
    "    max_len_d1 = -1\n",
    "    max_len_q2 = -1\n",
    "    max_len_d2 = -1\n",
    "\n",
    "    is_triplets = False\n",
    "    for elem in batch_objs:\n",
    "        if len(elem) == 3:\n",
    "            left_elem, right_elem, label = elem\n",
    "            is_triplets = True\n",
    "        else:\n",
    "            left_elem, label = elem\n",
    "\n",
    "        max_len_q1 = max(len(left_elem['query']), max_len_q1)\n",
    "        max_len_d1 = max(len(left_elem['document']), max_len_d1)\n",
    "        if len(elem) == 3:\n",
    "            max_len_q2 = max(len(right_elem['query']), max_len_q2)\n",
    "            max_len_d2 = max(len(right_elem['document']), max_len_d2)\n",
    "\n",
    "    q1s = []\n",
    "    d1s = []\n",
    "    q2s = []\n",
    "    d2s = []\n",
    "    labels = []\n",
    "\n",
    "    for elem in batch_objs:\n",
    "        if is_triplets:\n",
    "            left_elem, right_elem, label = elem\n",
    "        else:\n",
    "            left_elem, label = elem\n",
    "\n",
    "        pad_len1 = max_len_q1 - len(left_elem['query'])\n",
    "        pad_len2 = max_len_d1 - len(left_elem['document'])\n",
    "        if is_triplets:\n",
    "            pad_len3 = max_len_q2 - len(right_elem['query'])\n",
    "            pad_len4 = max_len_d2 - len(right_elem['document'])\n",
    "\n",
    "        q1s.append(left_elem['query'] + [0] * pad_len1)\n",
    "        d1s.append(left_elem['document'] + [0] * pad_len2)\n",
    "        if is_triplets:\n",
    "            q2s.append(right_elem['query'] + [0] * pad_len3)\n",
    "            d2s.append(right_elem['document'] + [0] * pad_len4)\n",
    "        labels.append([float(label)])\n",
    "    q1s = torch.LongTensor(q1s)\n",
    "    d1s = torch.LongTensor(d1s)\n",
    "    if is_triplets:\n",
    "        q2s = torch.LongTensor(q2s)\n",
    "        d2s = torch.LongTensor(d2s)\n",
    "    labels = torch.FloatTensor(labels)\n",
    "\n",
    "    ret_left = {'query': q1s, 'document': d1s}\n",
    "    if is_triplets:\n",
    "        ret_right = {'query': q2s, 'document': d2s}\n",
    "        return ret_left, ret_right, labels\n",
    "    else:\n",
    "        return ret_left, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fe1093e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 4)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7b24db5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11531, 4)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition = triplets[:, 3] == 0\n",
    "triplets[condition].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6a667f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(469, 4)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition = triplets[:, 3] == 1\n",
    "triplets[condition].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028eea34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c8e2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "227ddfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_vec = np.zeros_like((50,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4ec74777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fffde0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_vec = np.zeros((1, 50))\n",
    "oov_vec = np.random.uniform(low=-1,\n",
    "                            high=1,\n",
    "                            size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "520935a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2f2afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def __init__(self, glue_qqp_dir: str,\n",
    "                 glove_vectors_path: str,\n",
    "                 min_token_occurancies: int = 1,\n",
    "                 random_seed: int = 0,\n",
    "                 emb_rand_uni_bound: float = 0.2,\n",
    "                 freeze_knrm_embeddings: bool = True,\n",
    "                 knrm_kernel_num: int = 21,\n",
    "                 knrm_out_mlp: List[int] = [],\n",
    "                 dataloader_bs: int = 1024,\n",
    "                 train_lr: float = 0.1,\n",
    "                 change_train_loader_ep: int = 10\n",
    "                 ):\n",
    "        self.glue_qqp_dir = glue_qqp_dir\n",
    "        self.glove_vectors_path = glove_vectors_path\n",
    "        self.glue_train_df = self.get_glue_df('train')\n",
    "        self.glue_dev_df = self.get_glue_df('dev')\n",
    "        self.dev_pairs_for_ndcg = self.create_val_pairs(self.glue_dev_df)\n",
    "        self.min_token_occurancies = min_token_occurancies\n",
    "        self.all_tokens = self.get_all_tokens(\n",
    "            [self.glue_train_df, self.glue_dev_df], self.min_token_occurancies)\n",
    "\n",
    "        self.random_seed = random_seed\n",
    "        self.emb_rand_uni_bound = emb_rand_uni_bound\n",
    "        self.freeze_knrm_embeddings = freeze_knrm_embeddings\n",
    "        self.knrm_kernel_num = knrm_kernel_num\n",
    "        self.knrm_out_mlp = knrm_out_mlp\n",
    "        self.dataloader_bs = dataloader_bs\n",
    "        self.train_lr = train_lr\n",
    "        self.change_train_loader_ep = change_train_loader_ep\n",
    "\n",
    "        self.model, self.vocab, self.unk_words = self.build_knrm_model()\n",
    "        self.idx_to_text_mapping_train = self.get_idx_to_text_mapping(self.glue_train_df)\n",
    "        self.idx_to_text_mapping_dev = self.get_idx_to_text_mapping(self.glue_dev_df)\n",
    "\n",
    "        self.val_dataset = ValPairsDataset(self.dev_pairs_for_ndcg,\n",
    "                                           self.idx_to_text_mapping_dev,\n",
    "                                           vocab=self.vocab,\n",
    "                                           oov_val=self.vocab['OOV'],\n",
    "                                           preproc_func=self.simple_preproc)\n",
    "        self.val_dataloader = torch.utils.data.DataLoader(\n",
    "            self.val_dataset, batch_size=self.dataloader_bs, num_workers=0,\n",
    "            collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "    def get_glue_df(self, partition_type: str) -> pd.DataFrame:\n",
    "        assert partition_type in ['dev', 'train']\n",
    "        glue_df = pd.read_csv(self.glue_qqp_dir + f'/{partition_type}.tsv', sep='\\t', dtype=object)\n",
    "        glue_df = glue_df.dropna(axis=0, how='any').reset_index(drop=True)\n",
    "        glue_df_fin = pd.DataFrame({\n",
    "            'id_left': glue_df['qid1'],\n",
    "            'id_right': glue_df['qid2'],\n",
    "            'text_left': glue_df['question1'],\n",
    "            'text_right': glue_df['question2'],\n",
    "            'label': glue_df['is_duplicate'].astype(int)\n",
    "        })\n",
    "        return glue_df_fin\n",
    "\n",
    "    def handle_punctuation(self, inp_str: str) -> str:\n",
    "        # my code below\n",
    "        translator = str.maketrans(string.punctuation,\n",
    "                                   ' '*len(string.punctuation))\n",
    "        new_str = inp_str.translate(translator)\n",
    "        return new_str\n",
    "\n",
    "    def simple_preproc(self, inp_str: str) -> List[str]:\n",
    "        # my code below\n",
    "        no_punctuation_str = self.handle_punctuation(inp_str)\n",
    "        lowered_str = no_punctuation_str.strip().lower()\n",
    "        splitted_doc = nltk.word_tokenize(lowered_str)\n",
    "        return splitted_doc\n",
    "\n",
    "    def _filter_rare_words(self, vocab: Dict[str, int],\n",
    "                           min_occurancies: int) -> Dict[str, int]:\n",
    "        # my code below\n",
    "        filtered_vocab = {x: count for x, count in vocab.items() if\n",
    "                          count >= min_occurancies}\n",
    "        return filtered_vocab\n",
    "\n",
    "    def get_all_tokens(self, list_of_df: List[pd.DataFrame], min_occurancies: int) -> List[str]:\n",
    "        def flatten(t): return [item for sublist in t for item in sublist]\n",
    "        tokens = []\n",
    "        for df in list_of_df:\n",
    "            unique_texts = set(\n",
    "                df[['text_left', 'text_right']].values.reshape(-1))\n",
    "            df_tokens = flatten(map(self.simple_preproc, unique_texts))\n",
    "            tokens.extend(list(df_tokens))\n",
    "        count_filtered = self._filter_rare_words(\n",
    "            Counter(tokens), min_occurancies)\n",
    "        return list(count_filtered.keys())\n",
    "\n",
    "    def _read_glove_embeddings(self, file_path: str) -> Dict[str, List[str]]:\n",
    "        # my code below\n",
    "        with open(file_path, encoding='utf-8') as file:\n",
    "            glove_dict = {}\n",
    "            for line in file:\n",
    "                splitted_line = line.rstrip().split()\n",
    "                word, embedding = splitted_line[0], splitted_line[1:]\n",
    "                glove_dict[word] = embedding\n",
    "        return glove_dict\n",
    "\n",
    "    def create_glove_emb_from_file(self, file_path: str,\n",
    "                                   inner_keys: List[str],\n",
    "                                   random_seed: int,\n",
    "                                   rand_uni_bound: float\n",
    "                                   ) -> Tuple[np.ndarray,\n",
    "                                              Dict[str, int],\n",
    "                                              List[str]]:\n",
    "        # my code below\n",
    "        np.random.seed(random_seed)\n",
    "        glove_dict = self._read_glove_embeddings(file_path)\n",
    "        emb_dim = len(glove_dict['the'])\n",
    "\n",
    "        emb_matrix = []\n",
    "        pad_vec = np.zeros((emb_dim, ))\n",
    "        oov_vec = np.random.uniform(low=-rand_uni_bound,\n",
    "                                    high=rand_uni_bound,\n",
    "                                    size=emb_dim)\n",
    "        emb_matrix.append(pad_vec)\n",
    "        emb_matrix.append(oov_vec)\n",
    "\n",
    "        vocab = {}\n",
    "        unk_words = []\n",
    "        vocab['PAD'], vocab['OOV'] = 0, 1\n",
    "        for ind, token in enumerate(inner_keys, 2):\n",
    "            if token in glove_dict.keys():\n",
    "                emb_matrix.append(glove_dict[token])\n",
    "                vocab[token] = ind\n",
    "            else:\n",
    "                random_emb = np.random.uniform(low=-rand_uni_bound,\n",
    "                                               high=rand_uni_bound,\n",
    "                                               size=emb_dim)\n",
    "                emb_matrix.append(random_emb)\n",
    "                unk_words.append(token)\n",
    "                vocab[token] = ind\n",
    "        emb_matrix = np.array(emb_matrix).astype(float)\n",
    "        return emb_matrix, vocab, unk_words\n",
    "\n",
    "    def build_knrm_model(self) -> Tuple[\n",
    "            torch.nn.Module, Dict[str, int], List[str]]:\n",
    "        emb_matrix, vocab, unk_words = \\\n",
    "            self.create_glove_emb_from_file(self.glove_vectors_path,\n",
    "                                            self.all_tokens,\n",
    "                                            self.random_seed,\n",
    "                                            self.emb_rand_uni_bound)\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        knrm = KNRM(emb_matrix,\n",
    "                    freeze_embeddings=self.freeze_knrm_embeddings,\n",
    "                    out_layers=self.knrm_out_mlp,\n",
    "                    kernel_num=self.knrm_kernel_num)\n",
    "        return knrm, vocab, unk_words\n",
    "\n",
    "    def sample_data_for_train_iter(self, inp_df: pd.DataFrame, seed: int\n",
    "                                   ) -> List[List[Union[str, float]]]:\n",
    "        groups = inp_df[['id_left', 'id_right', 'label']].groupby('id_left')\n",
    "        pairs_w_labels = []\n",
    "        np.random.seed(seed)\n",
    "        all_right_ids = inp_df.id_right.values\n",
    "        for id_left, group in groups:\n",
    "            labels = group.label.unique()\n",
    "            if len(labels) > 1:\n",
    "                for label in labels:\n",
    "                    same_label_samples = group[group.label ==\n",
    "                                               label].id_right.values\n",
    "                    if label == 0 and len(same_label_samples) > 1:\n",
    "                        sample = np.random.choice(\n",
    "                            same_label_samples, 2, replace=False)\n",
    "                        pairs_w_labels.append(\n",
    "                            [id_left, sample[0], sample[1], 0.5])\n",
    "                    elif label == 1:\n",
    "                        less_label_samples = group[group.label <\n",
    "                                                   label].id_right.values\n",
    "                        pos_sample = np.random.choice(\n",
    "                            same_label_samples, 1, replace=False)\n",
    "                        if len(less_label_samples) > 0:\n",
    "                            neg_sample = np.random.choice(\n",
    "                                less_label_samples, 1, replace=False)\n",
    "                        else:\n",
    "                            neg_sample = np.random.choice(\n",
    "                                all_right_ids, 1, replace=False)\n",
    "                        pairs_w_labels.append(\n",
    "                            [id_left, pos_sample[0], neg_sample[0], 1])\n",
    "        return pairs_w_labels\n",
    "\n",
    "    def create_val_pairs(self,\n",
    "                         inp_df: pd.DataFrame,\n",
    "                         fill_top_to: int = 15,\n",
    "                         min_group_size: int = 2,\n",
    "                         seed: int = 0) -> List[List[Union[str, float]]]:\n",
    "        inp_df_select = inp_df[['id_left', 'id_right', 'label']]\n",
    "        inf_df_group_sizes = inp_df_select.groupby('id_left').size()\n",
    "        glue_dev_leftids_to_use = list(\n",
    "            inf_df_group_sizes[inf_df_group_sizes >= min_group_size].index)\n",
    "        groups = inp_df_select[inp_df_select.id_left.isin(\n",
    "            glue_dev_leftids_to_use)].groupby('id_left')\n",
    "\n",
    "        all_ids = set(inp_df['id_left']).union(set(inp_df['id_right']))\n",
    "\n",
    "        out_pairs = []\n",
    "\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        for id_left, group in groups:\n",
    "            ones_ids = group[group.label > 0].id_right.values\n",
    "            zeroes_ids = group[group.label == 0].id_right.values\n",
    "            sum_len = len(ones_ids) + len(zeroes_ids)\n",
    "            num_pad_items = max(0, fill_top_to - sum_len)\n",
    "            if num_pad_items > 0:\n",
    "                cur_chosen = set(ones_ids).union(\n",
    "                    set(zeroes_ids)).union({id_left})\n",
    "                pad_sample = np.random.choice(\n",
    "                    list(all_ids - cur_chosen), num_pad_items,\n",
    "                    replace=False).tolist()\n",
    "            else:\n",
    "                pad_sample = []\n",
    "            for i in ones_ids:\n",
    "                out_pairs.append([id_left, i, 2])\n",
    "            for i in zeroes_ids:\n",
    "                out_pairs.append([id_left, i, 1])\n",
    "            for i in pad_sample:\n",
    "                out_pairs.append([id_left, i, 0])\n",
    "        return out_pairs\n",
    "\n",
    "    def get_idx_to_text_mapping(self, inp_df: pd.DataFrame) -> Dict[str, str]:\n",
    "        left_dict = (\n",
    "            inp_df[\n",
    "                ['id_left', 'text_left']\n",
    "            ].drop_duplicates()\n",
    "            .set_index('id_left')\n",
    "            ['text_left'].to_dict()\n",
    "        )\n",
    "        right_dict = (\n",
    "            inp_df[\n",
    "                ['id_right', 'text_right']\n",
    "            ].drop_duplicates()\n",
    "            .set_index('id_right')\n",
    "            ['text_right'].to_dict()\n",
    "        )\n",
    "        left_dict.update(right_dict)\n",
    "        return left_dict\n",
    "\n",
    "    def _dcg(self, ys_true: np.array, ys_pred: np.array, k: int) -> float:\n",
    "        indices = np.argsort(-ys_pred)\n",
    "        ys_true = ys_true[indices[:k]]\n",
    "\n",
    "        sum_dcg = 0\n",
    "        for i, y_true in enumerate(ys_true, 1):\n",
    "            sum_dcg += (2 ** y_true - 1) / math.log2(i + 1)\n",
    "        return sum_dcg\n",
    "    \n",
    "    \n",
    "    def ndcg_k(self, ys_true: np.array, ys_pred: np.array,\n",
    "               ndcg_top_k: int = 10) -> float:\n",
    "        ideal_dcg = self._dcg(ys_true, ys_true, ndcg_top_k)\n",
    "        case_dcg = self._dcg(ys_true, ys_pred, ndcg_top_k)\n",
    "        return float(case_dcg / ideal_dcg)\n",
    "\n",
    "    def valid(self, model: torch.nn.Module,\n",
    "              val_dataloader: torch.utils.data.DataLoader) -> float:\n",
    "        labels_and_groups = val_dataloader.dataset.index_pairs_or_triplets\n",
    "        labels_and_groups = pd.DataFrame(labels_and_groups,\n",
    "                                         columns=['left_id', 'right_id',\n",
    "                                                  'rel'])\n",
    "\n",
    "        all_preds = []\n",
    "        for batch in (val_dataloader):\n",
    "            inp_1, y = batch\n",
    "            preds = model.predict(inp_1)\n",
    "            preds_np = preds.detach().numpy()\n",
    "            all_preds.append(preds_np)\n",
    "        all_preds = np.concatenate(all_preds, axis=0)\n",
    "        labels_and_groups['preds'] = all_preds\n",
    "\n",
    "        ndcgs = []\n",
    "        for cur_id in labels_and_groups.left_id.unique():\n",
    "            cur_df = labels_and_groups[labels_and_groups.left_id == cur_id]\n",
    "            ndcg = self.ndcg_k(cur_df.rel.values.reshape(-1),\n",
    "                               cur_df.preds.values.reshape(-1))\n",
    "            if np.isnan(ndcg):\n",
    "                ndcgs.append(0)\n",
    "            else:\n",
    "                ndcgs.append(ndcg)\n",
    "        return np.mean(ndcgs)\n",
    "    \n",
    "    def train(self, n_epochs: int):\n",
    "        opt = torch.optim.SGD(self.model.parameters(), lr=self.train_lr)\n",
    "        criterion = torch.nn.BCELoss()\n",
    "        for ep in range(n_epochs):\n",
    "            if ep % self.change_train_loader_ep == 0:\n",
    "                sampled_train_triplets = self.sample_data_for_train_iter(self.glue_train_df, seed=ep)\n",
    "                train_dataset = TrainTripletsDataset(sampled_train_triplets, \n",
    "                        self.idx_to_text_mapping_train, \n",
    "                        vocab=self.vocab, oov_val=self.vocab['OOV'], \n",
    "                        preproc_func=self.simple_preproc)\n",
    "                train_dataloader = torch.utils.data.DataLoader(\n",
    "                    train_dataset, batch_size=self.dataloader_bs, num_workers=0, \n",
    "                    collate_fn=collate_fn, shuffle=True,)\n",
    "            for j, data in enumerate(train_dataloader):\n",
    "                opt.zero_grad()\n",
    "                query_left_docs, query_right_docs, labels = data\n",
    "                outputs = self.model(query_left_docs, query_right_docs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "            val_ndcg = self.valid(self.model, self.val_dataloader)\n",
    "            print(f'Epoch: {ep}, validation ndcg {val_ndcg}')\n",
    "            if val_ndcg > 0.925:\n",
    "                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6b9ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "slt = Solution(glue_qqp_dir='data/QQP', glove_vectors_path='data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7f55ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, validation ndcg 0.8489403954532958\n",
      "Epoch: 1, validation ndcg 0.8367880379232742\n",
      "Epoch: 2, validation ndcg 0.9372785146576229\n"
     ]
    }
   ],
   "source": [
    "# 10 iteraions. NDCG starts from ~0.4\n",
    "slt.train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd357d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df28c965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
